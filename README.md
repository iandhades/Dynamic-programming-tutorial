# Dynamic-programming-tutorial

Dynamic programming is a approach for solving a complicated problem by breaking it down into a collection of simple subproblems. Solving each subproblem then storing its solution. When next time the same subproblems occurs, instead of recalculating the same question, it just simply looks at the previous computed solution which saves computation time and storage space. The method of looking at previously calculated answer is called “memoization”
						
Dynamic programming algorithms are mostly used for optimization, which examines the previously solved subproblems and combine their solution to find out the best solution for the given question. In contrast, greedy approach which has the same sequence as dynamic programming but it only picks the locally optimal answers at each step. Greedy approach doesn’t guarantee optimal solution and sometimes it may generate very bad answer. But some approaches used greedy approach such as Kruskal’s and Prim’s are proved to generate optimal solution.
						
For example, finding the minimum number of coins to make a given amount. By using dynamic programming, it would firstly finds an optimal solution for each smaller amount then using these solutions to construct an large amount. In comparison, greedy approach will start finding the largest possible coin of a given coin sequence, then at each step using the given amount minus the largest possible coin. For instance the given coin sequence is 1,4,5,15,20 and given amount is 23, the greedy approach will find 20+1+1+1, while the optimal solution is 15 +4 +4.
